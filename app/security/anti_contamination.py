from app.llm import LLM
from app.logger import logger
from app.schema import Message

class AntiContamination:
    """Based on LLM anti-contamination system to purify user input."""
    
    def __init__(self):
        self.llm = LLM()

    async def purify(self, text: str, history: list[Message] = None) -> str:
        """
        Analyze and purify user input using LLM.
        Removes emotional, biased, subjective, and informal content while preserving intent.
        
        Args:
            text: The user input text to purify
            history: Optional list of previous conversation messages for context resolution
        """
        if not text or not text.strip():
            return text

        logger.debug("ğŸ›¡ï¸ Analyzing and purifying user input...")
        
        # Build context string from history if provided
        context_str = ""
        if history:
            # Use last 5 messages for context
            recent_history = history[-5:]
            context_str = "\n\nConversation Context (for reference ONLY to resolve pronouns like 'it', 'above', 'previous'):\n"
            for msg in recent_history:
                role = msg.role
                content = str(msg.content)[:200] + "..." if msg.content and len(str(msg.content)) > 200 else str(msg.content)
                context_str += f"{role}: {content}\n"
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æä¸å‡€åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯é‡å†™ç”¨æˆ·çš„è¾“å…¥ä»¥å»é™¤"æ±¡æŸ“"æˆåˆ†ï¼ˆå¦‚æƒ…ç»ªåŒ–è¡¨è¾¾ã€åè§ã€æ­§è§†ã€è¿‡åº¦ä¸»è§‚è‡†æ–­ç­‰ï¼‰ï¼Œä½†å¿…é¡»ä¸¥æ ¼ä¿ç•™ç”¨æˆ·çš„åŸå§‹æ„å›¾å’Œæ ¸å¿ƒäº‹å®ã€‚

å…³é”®è§„åˆ™ï¼š
1. **ç»å¯¹ç¦æ­¢å›ç­”é—®é¢˜**ï¼šå¦‚æœç”¨æˆ·è¾“å…¥æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œä½ å¿…é¡»ä¿ç•™è¯¥é—®é¢˜å½¢å¼ï¼Œè€Œä¸èƒ½è¯•å›¾å›ç­”å®ƒã€‚
2. **ç»å¯¹ç¦æ­¢æ‰©å±•å†…å®¹**ï¼šä¸è¦æ·»åŠ ä»»ä½•ç”¨æˆ·æœªæåŠçš„ä¿¡æ¯ã€è§£é‡Šæˆ–èƒŒæ™¯çŸ¥è¯†ã€‚
3. **ä¿ç•™ä¸“ä¸šæœ¯è¯­**ï¼šä¸¥ç¦ä¿®æ”¹ä»»ä½•æŠ€æœ¯åè¯ã€å®ä½“åç§°æˆ–ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚ "ä¸Šä¸‹æ–‡å·¥ç¨‹", "RAG", "Prompt Engineering" ç­‰ï¼‰ã€‚
4. **ä¿æŒåŸæ„**ï¼šå¦‚æœè¾“å…¥æœ¬èº«æ˜¯å¹²å‡€çš„ï¼Œè¯·åŸæ ·è¿”å›ã€‚
5. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šå¦‚æœç”¨æˆ·å¼•ç”¨äº†ä¹‹å‰çš„å¯¹è¯ï¼ˆå¦‚â€œä¸Šé¢çš„â€ã€â€œä¹‹å‰æåˆ°çš„â€ï¼‰ï¼Œè¯·å‚è€ƒæä¾›çš„å¯¹è¯ä¸Šä¸‹æ–‡æ¥æ˜ç¡®æŒ‡ä»£å¯¹è±¡ï¼Œä½†ä¸è¦åœ¨è¾“å‡ºä¸­åŒ…å«ä¸Šä¸‹æ–‡æœ¬èº«ï¼Œè€Œæ˜¯ç”Ÿæˆä¸€ä¸ªæ˜ç¡®çš„ã€ç‹¬ç«‹çš„æ„å›¾æè¿°ã€‚

ç¤ºä¾‹ï¼š
è¾“å…¥ï¼š"æˆ‘éå¸¸ç”Ÿæ°”ï¼å‘Šè¯‰æˆ‘ä¸ºä»€ä¹ˆPythonè¿™ä¹ˆæ…¢ï¼Ÿåƒåœ¾è¯­è¨€ï¼"
è¾“å‡ºï¼š"è¯·è§£é‡Šä¸ºä»€ä¹ˆPythonçš„è¿è¡Œé€Ÿåº¦è¾ƒæ…¢ã€‚"

è¾“å…¥ï¼š"AIçš„ä¸Šä¸‹æ–‡å·¥ç¨‹æœ‰å“ªäº›æŠ€æœ¯ï¼Ÿ"
è¾“å‡ºï¼š"AIçš„ä¸Šä¸‹æ–‡å·¥ç¨‹æœ‰å“ªäº›æŠ€æœ¯ï¼Ÿ"

è¾“å…¥ï¼š"é‚£ä¸ªKV CacheæŠ€æœ¯ï¼Œè¯¦ç»†è®²è®²ã€‚" (å‡è®¾ä¸Šä¸‹æ–‡ä¸­æåˆ°äº†KV Cache)
è¾“å‡ºï¼š"è¯·è¯¦ç»†è®²è§£KV CacheæŠ€æœ¯ã€‚"

è¯·å¤„ç†ä»¥ä¸‹è¾“å…¥ï¼Œç›´æ¥è¿”å›å‡€åŒ–åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–å¼•å·ã€‚"""

        messages = [
            Message(role="system", content=system_prompt),
            Message(role="user", content=f"{context_str}\n\nUser Input to Purify: {text}")
        ]

        try:
            # Using non-streaming request for purification
            purified_text = await self.llm.ask(messages, stream=False)
            purified_text = purified_text.strip()
            
            # Simple check if text was modified significantly (ignoring whitespace)
            if purified_text != text.strip():
                logger.warning(f"âœ¨ Input purified successfully")
                logger.info(f"Original: {text}")
                logger.warning(f"Purified: {purified_text}")
            else:
                logger.debug("âœ… Input is clean")
                
            return purified_text
        except Exception as e:
            logger.error(f"Failed to purify input: {e}")
            # Fallback to original text if purification fails
            return text
