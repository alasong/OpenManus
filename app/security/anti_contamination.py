from app.llm import LLM
from app.logger import logger
from app.schema import Message

class AntiContamination:
    """Based on LLM anti-contamination system to purify user input."""
    
    def __init__(self):
        self.llm = LLM()

    async def purify(self, text: str) -> str:
        """
        Analyze and purify user input using LLM.
        Removes emotional, biased, subjective, and informal content while preserving intent.
        """
        if not text or not text.strip():
            return text

        logger.debug("ğŸ›¡ï¸ Analyzing and purifying user input...")
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æä¸å‡€åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ£€æµ‹ç”¨æˆ·è¾“å…¥ä¸­çš„"æ±¡æŸ“"æˆåˆ†ï¼ŒåŒ…æ‹¬ï¼š
1. æƒ…ç»ªåŒ–è¡¨è¾¾ï¼ˆæ„¤æ€’ã€ç„¦è™‘ã€æ‚²è§‚ç­‰ï¼‰
2. åè§ä¸æ­§è§†
3. è¿‡åº¦ä¸»è§‚è‡†æ–­
4. ä¸è§„èŒƒçš„è¡¨è¾¾ï¼ˆå¦‚æœå½±å“ç†è§£ï¼‰

è¯·é‡å†™ç”¨æˆ·çš„è¾“å…¥ï¼Œå»é™¤ä¸Šè¿°æ±¡æŸ“æˆåˆ†ï¼Œä¿æŒæ ¸å¿ƒäº‹å®å’Œæ„å›¾ä¸å˜ã€‚
å¦‚æœè¾“å…¥æœ¬èº«æ˜¯å¹²å‡€çš„ï¼Œè¯·åŸæ ·è¿”å›ï¼Œä¸è¦åšä»»ä½•ä¿®æ”¹ã€‚
ç›´æ¥è¿”å›æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€å¼•å·æˆ–é¢å¤–çš„å‰ç¼€/åç¼€ã€‚"""

        messages = [
            Message(role="system", content=system_prompt),
            Message(role="user", content=text)
        ]

        try:
            # Using non-streaming request for purification
            purified_text = await self.llm.ask(messages, stream=False)
            purified_text = purified_text.strip()
            
            # Simple check if text was modified significantly (ignoring whitespace)
            if purified_text != text.strip():
                logger.warning(f"âœ¨ Input purified successfully")
                logger.info(f"Original: {text}")
                logger.warning(f"Purified: {purified_text}")
            else:
                logger.debug("âœ… Input is clean")
                
            return purified_text
        except Exception as e:
            logger.error(f"Failed to purify input: {e}")
            # Fallback to original text if purification fails
            return text
